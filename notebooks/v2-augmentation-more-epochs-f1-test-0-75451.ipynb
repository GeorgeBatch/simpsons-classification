{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ**"},{"metadata":{},"cell_type":"markdown","source":"# Путешествие по Спрингфилду.\n\n\nСегодня вам предстоить помочь телекомпании FOX  в обработке их контента. Как вы знаете сериал Симсоны идет на телеэкранах более 25 лет и за это время скопилось очень много видео материала. Персоонажи менялись вместе с изменяющимися графическими технологиями   и Гомер 2018 не очень похож на Гомера 1989. Нашей задачей будет научиться классифицировать персонажей проживающих в Спрингфилде. Думаю, что нет смысла представлять каждого из них в отдельности.\n\n\n\n ![alt text](https://vignette.wikia.nocookie.net/simpsons/images/5/5a/Spider_fat_piglet.png/revision/latest/scale-to-width-down/640?cb=20111118140828)\n"},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{},"cell_type":"markdown","source":"## Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore deprication warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n\n# standard modules\nimport random\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, colors\n# work in interactive moode\n%matplotlib inline \n\n\n# loading files (in parallel)\nfrom pathlib import Path\nfrom multiprocessing.pool import ThreadPool\n\n\n# working with images\nimport PIL\nfrom PIL import Image\nfrom skimage import io\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n# torchvision\nimport torchvision\nfrom torchvision import transforms\n\n\n# interacrive timimg\nfrom tqdm import tqdm, tqdm_notebook\n\n# saving models \nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(PIL.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"torch.__version__ :\", torch.__version__)\nprint(\"torchvision.__version__ :\", torchvision.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choose training device"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will verify that GPU is enabled for this notebook\n# following should print: CUDA is available!  Training on GPU ...\n# \n# if it prints otherwise, then you need to enable GPU: \n# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Set device, "},{"metadata":{"trusted":true},"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE = 224\n# работаем на видеокарте\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1937\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the Data"},{"metadata":{},"cell_type":"markdown","source":"https://jhui.github.io/2018/02/09/PyTorch-Data-loading-preprocess_torchvision/"},{"metadata":{},"cell_type":"markdown","source":"Ниже мы исспользуем враппер над датасетом для удобной работы. Вам стоит понимать, что происходит с LabelEncoder и  с torch.Transformation. \n\nToTensor конвертирует  PIL Image с параметрами в диапазоне [0, 255] (как все пиксели) в FloatTensor размера (C x H x W) [0,1] , затем производится масштабирование:\n\n$input = \\frac{input - \\mu}{\\text{standard deviation}} $,\n\nконстанты - средние и дисперсии по каналам на основе ImageNet\n\n\nСтоит также отметить, что мы переопределяем метод __getitem__ для удобства работы с данной структурой данных.\n Также используется LabelEncoder для преобразования строковых меток классов в id и обратно. В описании датасета указано, что картинки разного размера, так как брались напрямую с видео, поэтому следуем привести их к одному размер (это делает метод  _prepare_sample) "},{"metadata":{},"cell_type":"markdown","source":"## Class for loading the data from folders in parallel"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n    \"\"\"\n    Class to work with image dastaset, which\n    - loads them form the folders in parallel\n    - converts to PyTorch tensors\n    - scales the tensors to have mean = 0, standard deviation = 1\n    \"\"\"\n    def __init__(self, files, mode):\n        super().__init__()\n        self.files = sorted(files) # list of files to be loaded\n        self.mode = mode           # working mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                \n    \n    def __len__(self):\n        return self.len_\n    \n    \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n    \n    \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)\n    \n    \n    def __getitem__(self, index):\n        # converts to PyTorch tensors and normalises the input\n        \n        \n        data_transforms = {\n            'train': transforms.Compose([\n                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n                transforms.RandomRotation(degrees=30),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(hue=.1, saturation=.1),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ]),\n            'val_test': transforms.Compose([\n                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ]),\n        }\n\n        transform = (data_transforms['train'] if self.mode == 'train' else data_transforms['val_test'])\n        \n        x = self.load_sample(self.files[index])  # load image\n        x = transform(x)                         # apply transform defined above\n        \n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow для тензоров\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get training filenames and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment to check the directories\n\n# %ls ../input/journey-springfield/testset/testset/\n# %ls ../input/journey-springfield/train/simpsons_dataset/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = Path('../input/journey-springfield/train/simpsons_dataset/')\nTEST_DIR = Path('../input/journey-springfield/testset/testset/')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*/*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_val_files), 'train files')\ntrain_val_files[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_files), 'test files')\ntest_files[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path.parent.name returns a folder in which the image is, which corresponds to the label in nthis case\ntrain_val_labels = [path.parent.name for path in train_val_files]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_val_labels), 'train_val_labels')\ntrain_val_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"## Train-Validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.25, stratify=train_val_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = SimpsonsDataset(val_files, mode='val')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the characters in the validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n                        sharey=True, sharex=True)\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Построение нейросети\n\nЗапустить данную сеть будет вашим мини-заданием на первую неделю, чтобы было проще участвовать в соревновании.\n\nДанная архитектура будет очень простой и нужна для того, чтобы установить базовое понимание и получить простенький сабмит на Kaggle\n\n<!-- Здесь вам предлагается дописать сверточную сеть глубины 4/5.  -->\n\n*Описание слоев*:\n\n\n\n1. размерность входа: 3x224x224 \n2. размерности после слоя:  8x111x111\n3. 16x54x54\n4. 32x26x26\n5. 64x12x12\n6. выход: 96x5x5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Очень простая сеть\nclass SimpleCnn(nn.Module):\n  \n    def __init__(self, n_classes):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.out = nn.Linear(96 * 5 * 5, n_classes)\n  \n  \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n\n        x = x.view(x.size(0), -1)\n        logits = self.out(x)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer):\n    \n    # initialize tracked variables\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        # reset the gradient\n        optimizer.zero_grad()\n        \n        # predictions (probabilities), loss, backprop\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # weights update\n        optimizer.step()\n        \n        # predictions (classes)\n        preds = torch.argmax(outputs, 1)\n        \n        # record tracked items\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n    \n    # record train loss and train accuracy\n    train_loss = running_loss / processed_data\n    train_acc = running_corrects.cpu().numpy() / processed_data\n    return train_loss, train_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion):\n    # set model model into the evaluation mode (e.g. for Dropout)\n    model.eval()\n    \n    # initialize tracked variables\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n        \n        # record tracked items\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n        \n    # record val loss and val accuracy\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        opt = torch.optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n        \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = len(np.unique(train_val_labels))\nsimple_cnn = SimpleCnn(n_classes).to(DEVICE)\nprint(\"we will classify {} classes:\\n\".format(n_classes))\nprint(simple_cnn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if val_dataset is None:\n    val_dataset = SimpsonsDataset(val_files, mode='val')\n    \ntrain_dataset = SimpsonsDataset(train_files, mode='train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = train(train_dataset, val_dataset, model=simple_cnn, epochs=5, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What now?"},{"metadata":{},"cell_type":"markdown","source":"![alt text](https://www.indiewire.com/wp-content/uploads/2014/08/the-simpsons.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Хорошо бы понять, как сделать сабмит. \nУ нас есть сеть и методы eval у нее, которые позволяют перевести сеть в режим предсказания. Стоит понимать, что у нашей модели на последнем слое стоит softmax, которые позволяет получить вектор вероятностей  того, что объект относится к тому или иному классу. Давайте воспользуемся этим."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"Предсказание, для одной картинки\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(simple_cnn, ex_img.unsqueeze(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(simple_cnn, imgs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare actual and predicted class ids"},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_labels = [val_dataset[id][1] for id in idxs]\nactual_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(probs_ims, -1)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обратите внимание, что метрика, которую необходимо оптимизировать в конкурсе - f1-score. Вычислим целевую метрику на валидационной выборке."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(actual_labels, y_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare actual and predicted classes (strings)"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_class = [label_encoder.classes_[i] for i in actual_labels]\nactual_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_class = [label_encoder.classes_[i] for i in y_pred]\npreds_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(actual_class, preds_class, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сделаем классную визуализацию, чтобы посмотреть насколько сеть уверена в своих ответах. Можете исспользовать это, чтобы отлаживать правильность вывода."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as patches\nfrom matplotlib.font_manager import FontProperties\n\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    \n    \n\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)\n    \n    actual_text = \"Actual : {}\".format(img_label)\n            \n    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n    font0 = FontProperties()\n    font = font0.copy()\n    font.set_family(\"fantasy\")\n    prob_pred = predict_one_sample(simple_cnn, im_val.unsqueeze(0))\n    predicted_proba = np.max(prob_pred)*100\n    y_pred = np.argmax(prob_pred)\n    \n    predicted_label = label_encoder.classes_[y_pred]\n    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n            \n    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Попробуйте найти те классы, которые сеть не смогла расспознать. Изучите данную проблему, это понадобится в дальнейшем."},{"metadata":{},"cell_type":"markdown","source":"# Submit to Kaggle competition"},{"metadata":{},"cell_type":"markdown","source":"![alt text](https://i.redd.it/nuaphfioz0211.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(simple_cnn, test_loader)\n\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check how the sample submission file looks"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/journey-springfield","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submit = pd.read_csv(\"../input/journey-springfield/sample_submission.csv\")\nsample_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nprint(my_submit.shape)\nmy_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submit.to_csv('baseline_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}