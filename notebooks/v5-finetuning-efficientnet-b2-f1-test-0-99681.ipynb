{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ**"},{"metadata":{},"cell_type":"markdown","source":"# Путешествие по Спрингфилду.\n\n\nСегодня вам предстоить помочь телекомпании FOX  в обработке их контента. Как вы знаете сериал Симсоны идет на телеэкранах более 25 лет и за это время скопилось очень много видео материала. Персоонажи менялись вместе с изменяющимися графическими технологиями и Гомер 2018 не очень похож на Гомера 1989. Нашей задачей будет научиться классифицировать персонажей проживающих в Спрингфилде. Думаю, что нет смысла представлять каждого из них в отдельности.\n\n\n ![alt text](https://vignette.wikia.nocookie.net/simpsons/images/5/5a/Spider_fat_piglet.png/revision/latest/scale-to-width-down/640?cb=20111118140828)\n"},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{},"cell_type":"markdown","source":"## Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore deprication warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n# standard python modules\nimport os, sys\nimport time\n\n\n# standard ml modules\nimport random\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, colors\n# work in interactive moode\n%matplotlib inline \n\n\n# loading files (in parallel)\nfrom pathlib import Path\nfrom multiprocessing.pool import ThreadPool\n\n\n# working with images\nimport PIL\nfrom PIL import Image\nfrom skimage import io\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n# torchvision\nimport torchvision\nfrom torchvision import transforms\n\n\n# interacrive timimg\nfrom tqdm import tqdm, tqdm_notebook\n\n# saving models \nimport pickle\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(PIL.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"torch.__version__ :\", torch.__version__)\nprint(\"torchvision.__version__ :\", torchvision.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choose training device"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will verify that GPU is enabled for this notebook\n# following should print: CUDA is available!  Training on GPU ...\n# \n# if it prints otherwise, then you need to enable GPU: \n# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Set device, "},{"metadata":{"trusted":true},"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE = 224\n# работаем на видеокарте\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the Data"},{"metadata":{},"cell_type":"markdown","source":"https://jhui.github.io/2018/02/09/PyTorch-Data-loading-preprocess_torchvision/"},{"metadata":{},"cell_type":"markdown","source":"Ниже мы исспользуем враппер над датасетом для удобной работы. Вам стоит понимать, что происходит с LabelEncoder и  с torch.Transformation. \n\nToTensor конвертирует  PIL Image с параметрами в диапазоне [0, 255] (как все пиксели) в FloatTensor размера (C x H x W) [0,1] , затем производится масштабирование:\n\n$input = \\frac{input - \\mu}{\\text{standard deviation}} $,\n\nконстанты - средние и дисперсии по каналам на основе ImageNet\n\n\nСтоит также отметить, что мы переопределяем метод __getitem__ для удобства работы с данной структурой данных.\n Также используется LabelEncoder для преобразования строковых меток классов в id и обратно. В описании датасета указано, что картинки разного размера, так как брались напрямую с видео, поэтому следуем привести их к одному размер (это делает метод  _prepare_sample) "},{"metadata":{},"cell_type":"markdown","source":"## Class for loading the data from folders in parallel"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n    \"\"\"\n    Class to work with image dastaset, which\n    - loads them form the folders in parallel\n    - converts to PyTorch tensors\n    - scales the tensors to have mean = 0, standard deviation = 1\n    \"\"\"\n    def __init__(self, files, mode):\n        super().__init__()\n        self.files = sorted(files) # list of files to be loaded\n        self.mode = mode           # working mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                \n    \n    def __len__(self):\n        return self.len_\n    \n    \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n    \n    \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)\n    \n    \n    def __getitem__(self, index):\n        # converts to PyTorch tensors and normalises the input\n        \n        data_transforms = {\n            'train': transforms.Compose([\n                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n                transforms.RandomRotation(degrees=30),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(hue=.1, saturation=.1),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ]),\n            'val_test': transforms.Compose([\n                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ]),\n        }\n\n        transform = (data_transforms['train'] if self.mode == 'train' else data_transforms['val_test'])\n        \n        x = self.load_sample(self.files[index])  # load image\n        x = transform(x)                         # apply transform defined above\n        \n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow для тензоров\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get training filenames and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment to check the directories\n\n# %ls ../input/journey-springfield/testset/testset/\n# %ls ../input/journey-springfield/train/simpsons_dataset/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = Path('../input/journey-springfield/train/simpsons_dataset/')\nTEST_DIR = Path('../input/journey-springfield/testset/testset/')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*/*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_val_files), 'train files')\ntrain_val_files[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_files), 'test files')\ntest_files[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path.parent.name returns a folder in which the image is, which corresponds to the label in nthis case\ntrain_val_labels = [path.parent.name for path in train_val_files]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_val_labels), 'train_val_labels')\ntrain_val_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"## Train-Validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.20, stratify=train_val_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = SimpsonsDataset(val_files, mode='val')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the characters in the validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n                        sharey=True, sharex=True)\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer):\n    # initialize tracked variables\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        # reset the gradient\n        optimizer.zero_grad()\n        \n        # predictions (probabilities), loss, backprop\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # weights update\n        optimizer.step()\n        \n        # predictions (classes)\n        preds = torch.argmax(outputs, 1)\n        \n        # record tracked items\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n        \n    # record train loss and train accuracy          \n    train_loss = running_loss / processed_data\n    train_acc = running_corrects.cpu().numpy() / processed_data\n    return train_loss, train_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion):\n    # set model model into the evaluation mode (e.g. for Dropout)\n    model.eval()\n    \n    # initialize tracked variables\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n        \n        # record tracked items\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n        \n    # record val loss and val accuracy\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_dataset, val_dataset, model, criterion,\n          epochs, batch_size, optimizer, scheduler,\n          shuffle=True, sampler=None, patience=5):\n    \n    # to record the total training time\n    since = time.time()\n    \n    # note: 4 workers loading the data\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    # init variables to store best model weights, best accuracy, best epoch number, epochs since best accuracy acheived\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 10\n    best_epoch = 0\n    epochs_since_best = 0\n    \n    # history and log\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n\n        for epoch in range(1, epochs+1):\n            print(f\"epoch {epoch}:\\n\")\n            \n            print(\"Fitting on train data...\")\n            # all arguments except train loader are from parameters passed to train() arguments\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer)\n            print(\"train loss:\", train_loss)\n            \n            print(\"Evaluating on validation data...\")\n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            print(\"val loss:\", val_loss)\n            \n            # record history\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            # update learning rate for the optimizer\n            scheduler.step()\n            \n            # display learning status\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n            # deep copy the model if it acheives the best validation performance\n            if val_loss < best_loss:\n                best_acc = val_loss\n                best_epoch = epoch\n                best_model_wts = copy.deepcopy(model.state_dict())\n                print()\n            else:\n                epochs_since_best += 1\n            \n            # early stopping\n            if epochs_since_best > patience:\n                print(f'Stopping training. The validation metric has not improved for {patience} epochs.')\n                break\n            \n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n    print('Best val loss: {:4f}'.format(best_loss))\n    print('Best epoch: {}'.format(best_epoch))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n            \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n        \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training: fine-tuning pretrained resnet18"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_CLASSES = len(np.unique(train_val_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if val_dataset is None:\n    val_dataset = SimpsonsDataset(val_files, mode='val')\n    \ntrain_dataset = SimpsonsDataset(train_files, mode='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training only the last layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'efficientnet-b2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = torchvision.models.resnet18(pretrained=True)\nmodel = EfficientNet.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model._fc.in_features\nmodel._fc = nn.Linear(num_ftrs, N_CLASSES)\n\n# to GPU\nmodel = model.to(DEVICE)\n\n# loss\ncriterion = nn.CrossEntropyLoss()\n\n# learning rate optimizer\noptimizer = torch.optim.AdamW(model.parameters())\n\n# scheduler for the lr optimizer\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model._fc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_extr_epochs = 1 # test run\n# feature_extr_epochs = 3 # performance run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_feature_extr = train(train_dataset, val_dataset, model=model, criterion=criterion,\n                             epochs=feature_extr_epochs, batch_size=256, optimizer=optimizer, scheduler=scheduler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history_feature_extr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training all layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finetuning_epochs = 1 # test run\n# finetuning_epochs = 50 # performance run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_fine_tune = train(train_dataset=train_dataset, val_dataset=val_dataset, model=model, criterion=criterion,\n                          epochs=finetuning_epochs, batch_size=16, optimizer=optimizer, scheduler=scheduler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history_fine_tune)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\n\n\nplt.savefig(f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-LearningCurve.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-LearningCurve.png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the weights of our net\nmodel_weights = copy.deepcopy(model.state_dict())\ntorch.save(model_weights, f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-weights.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# загружаем сохраненное состояние весов нейросети\nmodel.load_state_dict(torch.load(f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-weights.pth\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What now?"},{"metadata":{},"cell_type":"markdown","source":"![alt text](https://www.indiewire.com/wp-content/uploads/2014/08/the-simpsons.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Хорошо бы понять, как сделать сабмит. \nУ нас есть сеть и методы eval у нее, которые позволяют перевести сеть в режим предсказания. Стоит понимать, что у нашей модели на последнем слое стоит softmax, которые позволяет получить вектор вероятностей  того, что объект относится к тому или иному классу. Давайте воспользуемся этим."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"Предсказание, для одной картинки\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(model, ex_img.unsqueeze(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare actual and predicted class ids"},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_labels = [val_dataset[id][1] for id in idxs]\nactual_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(probs_ims, -1)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обратите внимание, что метрика, которую необходимо оптимизировать в конкурсе - f1-score. Вычислим целевую метрику на валидационной выборке."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(actual_labels, y_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare actual and predicted classes (strings)"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_class = [label_encoder.classes_[i] for i in actual_labels]\nactual_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_class = [label_encoder.classes_[i] for i in y_pred]\npreds_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(actual_class, preds_class, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сделаем классную визуализацию, чтобы посмотреть насколько сеть уверена в своих ответах. Можете исспользовать это, чтобы отлаживать правильность вывода."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as patches\nfrom matplotlib.font_manager import FontProperties\n\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    \n    \n\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)\n    \n    actual_text = \"Actual : {}\".format(img_label)\n            \n    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n    font0 = FontProperties()\n    font = font0.copy()\n    font.set_family(\"fantasy\")\n    prob_pred = predict_one_sample(model, im_val.unsqueeze(0))\n    predicted_proba = np.max(prob_pred)*100\n    y_pred = np.argmax(prob_pred)\n    \n    predicted_label = label_encoder.classes_[y_pred]\n    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n            \n    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Попробуйте найти те классы, которые сеть не смогла расспознать. Изучите данную проблему, это понадобится в дальнейшем."},{"metadata":{},"cell_type":"markdown","source":"# Submit to Kaggle competition"},{"metadata":{},"cell_type":"markdown","source":"![alt text](https://i.redd.it/nuaphfioz0211.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64, num_workers=4)\nprobs = predict(model, test_loader)\n\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check how the sample submission file looks"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/journey-springfield","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submit = pd.read_csv(\"../input/journey-springfield/sample_submission.csv\")\nsample_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nprint(my_submit.shape)\nmy_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submit.to_csv(f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Names of saved files"},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-weights.pth\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-LearningCurve.png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{model_name}_{feature_extr_epochs}FeatureExtrEpochs-{finetuning_epochs}FinetuningEpochs-submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}